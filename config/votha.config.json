{
  "model": {
    "provider": "ollama",
    "name": "llama3.2:8b",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 40
  },
  "stream": {
    "default_state": "engaged",
    "chat_lull_threshold": 5,
    "chat_spike_threshold": 30
  },
  "overlay": {
    "ws_url": "ws://localhost:8765"
  },
  "behavior": {
    "allow_repeat_after_seconds": 600
  }
}
